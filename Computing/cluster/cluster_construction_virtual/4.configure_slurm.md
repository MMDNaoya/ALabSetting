

# slurmのインストール
```

#mpi関連をインストール
sudo apt install libgomp1 openmpi-bin libopenmpi-dev

sudo apt install slurm-wlm
# バージョンが異なるとめんどくさいので勝手に更新されないようにする
sudo echo slurm-wlm hold | sudo dpkg --set-selections

sudo mkdir -p /var/log/slurm # ログを入れるディレクトリを作成
sudo chown -R slurm:slurm /var/log/slurm # によってslurmがアクセスできるようにする
sudo mkdir -p /var/spool/slurmctld # ほとんど同様
sudo chown -R slurm:slurm /var/spool/slurmctld
```

## controllerの設定

### slurmのconfigファイル作成
slurmの設定は```/etc/slurm/slurm.conf```に記述する． ```slurm.conf```を作成するために，slurmが作ったツールを使える．slurmのパッケージをgithubからダウンロードし，その中から設定ツールを取り出せる．執筆時点でのバージョンは[slurm23.11.4-1.2](https://github.com/SchedMD/slurm/archive/refs/tags/slurm-23-11-4-1.zip)であるので，それをダウンロードする．　　
解凍して```doc/html/configurator.html.in```をブラウザで開くと設定ツールを開ける．設定ツールが出力した設定の雛形をエディタで編集していくのがいいと思う．  
画像1の通りにポチポチして編集した結果が以下である．

```

ClusterName=cluster
SlurmctldHost=controller
AuthType=auth/slurm
CredType=cred/slurm
MpiDefault=none
ProctrackType=proctrack/cgroup
ReturnToService=1
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmctldPort=6817
SlurmdPidFile=/var/run/slurmd.pid
SlurmdPort=6818
SlurmdSpoolDir=/var/spool/slurmd
SlurmUser=slurm
StateSaveLocation=/var/spool/slurmctld
TaskPlugin=task/affinity,task/cgroup
SlurmctldParameters=enable_configless

# TIMERS
InactiveLimit=0
KillWait=30
MinJobAge=300
SlurmctldTimeout=120
SlurmdTimeout=300
Waittime=0

# メールを送信しない
MailProg=/bin/true

# SCHEDULING
SchedulerType=sched/backfill
SelectType=select/cons_tres
SelectTypeParameters=CR_CORE

# LOGGING AND ACCOUNTING
JobCompType=jobcomp/none
JobAcctGatherFrequency=30
SlurmctldDebug=info
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdDebug=info
SlurmdLogFile=/var/log/slurm/slurmd.log

# POWER SAVE SUPPORT FOR IDLE NODES (optional)
#SuspendProgram=
#ResumeProgram=
#SuspendTimeout=
#ResumeTimeout=
#ResumeRate=
#SuspendExcNodes=
#SuspendExcParts=
#SuspendRate=
#SuspendTime=

# COMPUTE NODES
NodeName=controller CPUs=2 Sockets=1 CoresPerSocket=2 ThreadsPerCore=1 State=UNKNOWN
NodeName=epyc1 CPUs=3 Sockets=1 CoresPerSocket=3 ThreadsPerCore=1 State=UNKNOWN
NodeName=acpu1-[1-3] CPUs=2 Sockets=1 CoresPerSocket=2 ThreadsPerCore=1 State=UNKNOWN
# acpu1-[1-3]はacpu1-1, acpu1-2, acpu1-3をまとめて書く方法
#NodeName=acpu1-1 CPUs=1 Sockets=1 CoresPerSocket=1 ThreadsPerCore=1 State=UNKNOWN
#NodeName=acpu1-2 CPUs=1 Sockets=1 CoresPerSocket=1 ThreadsPerCore=1 State=UNKNOWN
#NodeName=acpu1-3 CPUs=1 Sockets=1 CoresPerSocket=1 ThreadsPerCore=1 State=UNKNOWN
PartitionName=all Nodes=ALL Default=YES OverSubscribe=YES MaxTime=INFINITE State=UP
PartitionName=exclusives Nodes=controller,acpu1-[1-3] OverSubscribe=YES MaxTime=INFINITE State=UP
```

epyc1ノードは4coresの設定で作成したとかのことを踏まえて更に編集したところ．

## 共通設定
- キーの共有  
slurmのノード間の認証のために全てのノードで```src/slurm.key```を```/etc/slurm/```以下に配置する．```src/slurm.key```を```/cluster/share/installation```に配置している場合
```
sudo cp /cluster/share/installation/slurm.key /etc/slurm

# さらにパーミッションを調整
sudo chown slurm:slurm /etc/slurm/slurm.key 
sudo mod 600 /etc/slurm/slurm.key
```

- クラスタのipとホスト名の対応づけ  
/etc/hostsは，ホスト名とipアドレスの対応付を保存しておける．この仮想環境のホストたちをここに書いておく．

```

# localhostは必ず127.0.0.0.1
127.0.0.1 localhost
# 127.0.1.1は[仕様バグらしい](https://qiita.com/naohiro2g/items/a9c997d8045f7ab75f9f)のでコメントアウト
# 127.0.1.1 controller
200.0.1.1 acpu1-1
200.0.1.2 acpu1-2
200.0.1.3 acpu1-3
192.168.1.200 controller
192.168.1.14 epyc1

# ip6v関係はそのまま
::1     ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters

```

- controllerがヘッドノードであると全ノードに知らせる設定  
controllerのslurm.confファイルを計算ノードに読ませるため，すべての計算ノードで
```/etc/default/slurmd```に
```SLURMD_OPTIONS="--conf-server controller"```を追記する．

## 設定がうまくできていることを確認
### controller
```sudo systemctl restart slurmctld``` によってslurmのコントローラープログラム(slurmctld)を再起動し，これまでに行った設定を反映させる．

失敗する場合，```sudo systemctl status slurmctld```でエラーの詳細が見える．パーミッションのエラーや作成すべきディレクトリを作成し忘れることで生じるエラーが多い．

### worker
```sudo systemctl restart slurmd```