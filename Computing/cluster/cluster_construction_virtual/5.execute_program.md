# 準備

- slurmが正常に動いていることの確認
```sinfo```で以下のような感じになっていれば成功．
```
PARTITION  AVAIL  TIMELIMIT  NODES  STATE NODELIST
all*          up   infinite      5   idle acpu1-[1-3],controller,epyc1
exclusives    up   infinite      4   idle acpu1-[1-3],controller
```

- アウトプットディレクトリ作成
```
mkdir -p /cluster/archive/user1/project1/resutls /cluster/archive/user1/project1/logs/out /cluster/archive/user1/project1/logs/err
```
によって，プログラム実行による
- 正常系の結果
- 正常系のログ
- 異常系のログ

を書き出すディレクトリを作成する．

- プログラムディレクトリ作成  
```./src```を```/cluster/share/user1/project1```にコピー


# サンプルスクリプト

controllerノードで```sbatch sample.sh``` を実行することで，ジョブをslurmに登録する．

## 単純なbash
### コアごとの実行

```

#!/bin/bash
#SBATCH --job-name sample1_echo_hostname
#SBATCH --output /cluster/archive/user1/project1/logs/out/%x-%j.log
#SBATCH --error  /cluster/archive/user1/project1/logs/err/%x-%j.err
#SBATCH --cpus-per-task 1
#SBATCH --time 00:15
#SBATCH --partition all
sleep 10
hostname_=$(hostname)
echo ${hostname_} >> /cluster/archive/user1/project1/resutls/result-sample.sh

```

```sample1.sh```はbashスクリプトなので```#```はコメントアウトの記号なのだが，先頭が```#SBATCH```である行はslurmが```sbatch```実行のオプションを表す文字列として解釈する．例えば```#SBATCH --job-name hostname```という行は```--job-name hostname```というオプションをsbatchコマンドに与えている．  

```sample1.sh```の各行の意味は以下のとおりである．なお，%xはジョブ名、%j はジョブIDに置換される．
1. #!/bin/bash  
このスクリプトが Bash で実行されることを指定する「シバン（shebang）」行．

2. #SBATCH --job-name hostname

ジョブの名前を hostname に設定．
sbatch でジョブを投入したときに、この名前で識別される．

3. #SBATCH --output /cluster/archive/user1/project1/logs/out/%x-%j.log

標準出力（正常系ログ） を /cluster/archive/user1/project1/logs/out/%x-%j.log 以下に保存．
%x はジョブ名、%j はジョブIDに置き換えられる．
例: hostname-123456.log

4. SBATCH --error  /cluster/archive/user1/project1/logs/err/%x-%j.err

標準エラー出力（異常系ログ） を /cluster/archive/user1/project1/logs/err/%x-%j.err に保存．
%x-%j により、ジョブごとのエラーログが区別される．

5. #SBATCH --cpus-per-task 1

ジョブに割り当てる CPU コア数を 1 に設定．

6. #SBATCH --time 00:15

ジョブの最大実行時間 を 15分間に制限．
制限時間を超えるとジョブは強制終了される．

7. #SBATCH --partition all

ジョブを投入するパーティション を ```all``` に設定．
クラスタの管理者が定義した特定のノードグループでジョブを実行する．

8. sleep 10

10秒間スリープ し、ジョブの動作を遅延させる．

9. hostname_=$(hostname)

現在のノードのホスト名 を取得し、変数 hostname_ に保存．

10. echo ${hostname_} >> /cluster/archive/user1/project1/resutls/result-sample.sh

ホスト名を/cluster/archive/user1/project1/resutls/result-sample.sh に追記 する．
実行された計算ノードのホスト名を記録するために使用される．

20回```sbatch sample1.sh```を実行することで```/cluster/archive/user1/project1/resutls/result-sample.sh```に以下のような結果が得られた．クラスタ内のそれぞれのノードで，コアの数だけ並列実行できているのがわかる．  
```
controller
controller
epyc1
epyc1
epyc1
acpu1-3
acpu1-2
acpu1-1
controller
acpu1-2
acpu1-3
(つづく)
```

### ジョブ実行のコア数指定


また，```squeue```コマンドによって投入した未完了のジョブの一覧を見ることができ，画像のように各ノードのコア数のぶんだけジョブが並列実行されているとわかる．![](figures/slurmparallel.png)

## pythonスクリプト
```uv```や```pipenv```によって```/cluster/share/user1/project2/```に仮想環境を作成し，その環境をノード間で共有してプログラムを実行する方法を示す．

```/cluster/share/user1/project2/```で```uv init --virtual```によって仮想環境を作成し，```uv add numpy```によって仮想環境にnumpyを追加する．  

```sample2.py```を
```
import sys

def main():
    print("sys.executable:", sys.executable)
    import numpy as np
    print("np.__file__: ", np.__file__)

if __name__ == "__main__":
    main()
```

とする．これを実行するとpythonの実行環境とnumpyの場所をプリントする．numpyが見えない環境では当然エラーが生じる．

さらに```sample2.py```を実行するためのシェルスクリプト```sample2_success.sh```，```sample2_error.sh```を以下に示す．違いは後者では```source /cluster/share/user1/project2/.venv/bin/activate```によって作成した仮想環境を有効化している点である．```sample2_success.sh```は正常に完了し，ログが
```
sys.executable: /cluster/share/user1/project2/.venv/bin/python3
np.__file__:  /cluster/share/user1/project2/.venv/lib/python3.12/site-packages/numpy/__init__.py
```
が```/cluster/archive/user1/project2/logs/out/python_import-61.log``` に出力されるはずである．


一方```sample_error.sh```は失敗し，失敗するまでのログが
```/cluster/archive/user1/project2/logs/out/python_import-62.log```に
```
sys.executable: /usr/bin/python3
```

エラーログが

```/cluster/archive/user1/project2/logs/err/python_import-62.err```:  
```
Traceback (most recent call last):
  File "/cluster/share/user1/project2/sample2.py", line 9, in <module>
    main()
  File "/cluster/share/user1/project2/sample2.py", line 5, in main
    import numpy as np
ModuleNotFoundError: No module named 'numpy'
```
と出る．

#### sample_success.sh
```
#!/bin/bash
#SBATCH --job-name hostname
#SBATCH --output /cluster/archive/user1/project2/logs/out/%x-%j.log
#SBATCH --error  /cluster/archive/user1/project2/logs/err/%x-%j.err
#SBATCH --cpus-per-task 1
#SBATCH --time 00:15
source /cluster/share/user1/project2/.venv/bin/activate
python3 /cluster/share/user1/project2/main.py
deactivate
```

#### sample_error.sh
```
#!/bin/bash
#SBATCH --job-name hostname
#SBATCH --output /cluster/archive/user1/project2/logs/out/%x-%j.log
#SBATCH --error  /cluster/archive/user1/project2/logs/err/%x-%j.err
#SBATCH --cpus-per-task 1
#SBATCH --time 00:15
#source /cluster/share/user1/project2/.venv/bin/activate
python3 /cluster/share/user1/project2/main.py
#deactivate
```

## FortranやC
実行プログラムを/cluster/share以下に置いて実行する方法があるが，各ノードでCPUのベンダー自体が異なるので，それぞれのノードでコンパイルを行う方法を記しておく．運が良ければ5%くらい高速化できるかも．  
AMD, Intel両方の謹製コンパイラとGCCが入っているので，ノードが自分のCPUベンダーを判別して適したコンパイラを選ぶロジックをどこかしらに仕込むか，gccにおまかせすることになる．  

例えば適当なsample3.f90というコードをノードごとにコンパイルするとき，```sbatch```に与えるbashファイルに

```
#SBATCH --job-name=my_job  # デフォルトのジョブ名（必要に応じて変更）

# ジョブ名とジョブIDの取得
JOB_NAME=${SLURM_JOB_NAME}
JOB_ID=${SLURM_JOB_ID}

# 出力ファイル名
OUTPUT_FILE="sample3_${JOB_NAME}_${JOB_ID}.out"

# CPUベンダーの取得
VENDOR=$(lscpu | awk -F: '/Vendor ID/ {print $2}' | xargs)

# デフォルトのコンパイラ設定
if [[ "$VENDOR" == "GenuineIntel" ]]; then
    COMPILER="ifx"
elif [[ "$VENDOR" == "AuthenticAMD" ]]; then
    COMPILER="ifort"
else
    COMPILER="gfortran"
fi

# コンパイル
$COMPILER -O3 -march=native sample3.f90 -o "$OUTPUT_FILE" 2>/dev/null

# 失敗時にgfortranを使用
if [[ $? -ne 0 ]]; then
    echo "Failed with $COMPILER, retrying with gfortran..."
    gfortran -O3 -march=native sample3.f90 -o "$OUTPUT_FILE"
    if [[ $? -ne 0 ]]; then
        echo "Compilation failed with all compilers."
        exit 1
    fi
fi

# sample3.outを実行
# 実行可能ファイルを実行
chmod +x "$OUTPUT_FILE"
./"$OUTPUT_FILE"
```
のようにする．



## クラスタのパーティション
```/etc/slurm/slurm.conf```の最後の方に記述した
```
PartitionName=all Nodes=ALL Default=YES OverSubscribe=YES MaxTime=INFINITE State=UP
PartitionName=exclusives Nodes=controller,acpu1-[1-3] OverSubscribe=YES MaxTime=INFINITE State=UP
```
はクラスタの論理的な分割を示す．```all```ではepyc1も含めているが，epyc1が他に優先的な計算を行っている場合にはクラスタ専属ノードのみを使う```exclusives```パーティションを用いるなど便利な状況がある．他にもGPUを積んだノードは除くとか，GPUを積んだノードのみ使うとか考えられる．
その際には```sbatch```の引数であるスクリプトファイルに```#SBATCH --partition exclusives```と書くとか，```sbatch```のオプションに```--partition exclusives```を指定するなどすればよい．


## 補遺
pythonの仮想環境を共有ディレクトリ```/cluster/share```下に配置した一方，コンパイラ含めほかのプログラムは各ノードのローカルに保存されている．これはpythonの仮想環境は更新頻度が高いと見込まれ，更新時にいちいち全てのノードを更新するのが面倒だからである．全てのプログラムを共有ディレクトリに配置する構成も考えたのだが，ネットワーク速度の制約に加え，ユーザ設定をクラスタ内の全ノードで統一する手間が要ったり，CPUがintelだったりamdだったりする実環境でそれをして不具合が起きないか不安だったので見送った． 